# Documenting Code

1. I started by creating the basic project structure including folders like src , docs etc and the git repository.

2. I created a virtual environment and installed the necessary packages.

3. Then began by uplaoding the data to the s3 bucket using aws python sdk through boto3 made the code modular and reusable.

4. Created another module in src for data ingestion and created functions to load the data from s3 bucket and initialize sparksession. 

5. Created a .env file to store the environment variables.

6. Went to Iam console in aws and got the access key with getobject and putobject permissions (s3 bucket)

7. Studed about hadoop-aws integration through s3a:// protocol and the spark.jars.packages to include the hadoop-aws package.

8. Then started creating the notebook sessions and basic data analysis like schema etc

9. Created a new module in src for data cleaning and created functions to clean the data.
    1- Null Handling Logic
        * If a column has >20% nulls:
            - Drop the entire column.
        
        * If a column has <20% nulls:
            - Drop the rows where the column is null.

            (implemented a verbose for all this as well)
        * Reason why imputation is not done:-
            - The data is related to flights which is sensitive data and imputation would introduce bias into the data.

10. Learnt about brodcast joins and its implementation:-
    - Broadcast joins are used to join a small DataFrame with a large DataFrame when the small DataFrame is broadcasted to every worker node in the cluster.
    - This is used to avoid shuffling the data and to improve the performance of the join operation.

11. Learnt about repartitioning and its implementation:-
    - Repartitioning is used to change the number of partitions in the DataFrame.
    - This is used to improve the performance of the join operation.
    - The number of partitions is changed by using the repartition function.

12. Learnt about UDFs and their implementation:-
    - UDFs are used to create custom functions in PySpark but native pyspark functions are faster and more efficient.

13. Completed the basic EDA notebook and saved A dataframe output  to s3 bucket.

14. Implemented complete readme and EDA notebook with images to make it more informative and engaging.

15. Learnt about Optimization techniques in pyspark while dealing with Big data we need these :-
    - Use df.select('col name') instead of '*' reduces data shuflling and memory usage. 
    - Apply .filter() or .where() as early as possible,  reduces the amount of data read from disk and minimizes shuffling       across the network. Less data = faster execution.
    - Avoid UDFs unless absolutely necessary , use pyspark built in functions as they are more efficient
    - Use .cache() only when dataframe is reused multiple times , avoid caching large datasets unless needed.
    - Avoid costly shuffles in big-small table joins , by using brodcast joins as it avoids shuffle by sending small     
      dataset to each worker node
    - Repartition data for parallelism: df.repartition("col") , Good partitioning ensures tasks are evenly distributed           across workers. Without it, some workers process more data (“data skew”), causing longer execution times.
    - Never use collect() or toPandas() on large datasets , These actions move data to the driver, which can cause Out of         Memory (OOM) errors if the dataset is too big.
    
    






















    
